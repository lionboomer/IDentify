{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"fingerprintDB\"]\n",
    "collection = db[\"fingerprints\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, target_size):\n",
    "    print(\"Resizing image...\")\n",
    "    resized_image = np.array(Image.fromarray(image).resize(target_size, Image.LANCZOS))\n",
    "    print(\"Image resized.\")\n",
    "    return resized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, target_size):\n",
    "    return np.array(Image.fromarray(image).resize(target_size, Image.LANCZOS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (35, 280)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing user: username_1\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Decoding and resizing canvas...\n",
      "Normalizing canvas data...\n",
      "Encoding labels...\n",
      "Splitting data into training and test sets...\n",
      "Creating model...\n",
      "Compiling model...\n",
      "Training model...\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Saving model...\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "users_data = collection.find()\n",
    "for user in users_data:\n",
    "    print(f\"Processing user: {user['username']}\")\n",
    "    canvases = []\n",
    "    labels = []\n",
    "    for canvas in user[\"canvases\"]:\n",
    "        print(\"Decoding and resizing canvas...\")\n",
    "        decoded_image = decode_canvas(canvas)\n",
    "        print(f\"Decoded image shape: {decoded_image.shape}\")\n",
    "        resized_image = resize_image(decoded_image, target_size)\n",
    "        print(f\"Resized image shape: {resized_image.shape}\")\n",
    "        canvases.append(resized_image)\n",
    "        labels.append(user[\"username\"])\n",
    "    \n",
    "    print(\"Normalizing canvas data...\")\n",
    "    canvases = np.array(canvases) / 255.0\n",
    "    print(f\"Canvases shape: {canvases.shape}\")\n",
    "    labels = np.array(labels)\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    # Labels in numerische Werte umwandeln\n",
    "    print(\"Encoding labels...\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    print(f\"Encoded labels shape: {labels.shape}\")\n",
    "    \n",
    "    # Daten in Trainings- und Testdaten aufteilen\n",
    "    print(\"Splitting data into training and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(canvases, labels, test_size=0.2, random_state=42)\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    # Modell erstellen\n",
    "    print(\"Creating model...\")\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(280, 35, 4)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Modell kompilieren\n",
    "    print(\"Compiling model...\")\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Modell trainieren\n",
    "    print(\"Training model...\")\n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Modell speichern\n",
    "    print(\"Saving model...\")\n",
    "    model.save(f'model_{user[\"username\"]}.h5')\n",
    "    print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Test loss: 0.0\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Modell evaluieren\n",
    "evaluation = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {evaluation[0]}\")\n",
    "print(f\"Test accuracy: {evaluation[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Test loss: 0.0\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# MongoDB-Verbindung herstellen\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"fingerprintDB\"]\n",
    "collection = db[\"fingerprints\"]\n",
    "\n",
    "def decode_canvas(canvas_data):\n",
    "    image_data = base64.b64decode(canvas_data.split(\",\")[1])\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "    return np.array(image)\n",
    "\n",
    "def resize_image(image, target_size):\n",
    "    return np.array(Image.fromarray(image).resize(target_size, Image.LANCZOS))\n",
    "\n",
    "target_size = (35, 280)\n",
    "\n",
    "users_data = collection.find()\n",
    "for user in users_data:\n",
    "    canvases = []\n",
    "    labels = []\n",
    "    for canvas in user[\"canvases\"]:\n",
    "        decoded_image = decode_canvas(canvas)\n",
    "        resized_image = resize_image(decoded_image, target_size)\n",
    "        canvases.append(resized_image)\n",
    "        labels.append(user[\"username\"])\n",
    "    \n",
    "    canvases = np.array(canvases) / 255.0\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Labels in numerische Werte umwandeln\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    # Daten in Trainings- und Testdaten aufteilen\n",
    "    X_train, X_test, y_train, y_test = train_test_split(canvases, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Modell erstellen\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(280, 35, 4)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Modell kompilieren\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Modell trainieren\n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Modell evaluieren\n",
    "    evaluation = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test loss: {evaluation[0]}\")\n",
    "    print(f\"Test accuracy: {evaluation[1]}\")\n",
    "    \n",
    "    # Modell speichern\n",
    "    model.save(f'model_{user[\"username\"]}.h5')\n",
    "\n",
    "    # Modell in TensorFlow.js umwandeln\n",
    "    import os\n",
    "    os.system(f'tensorflowjs_converter --input_format=keras model_{user[\"username\"]}.h5 /path/to/tfjs_model_{user[\"username\"]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
